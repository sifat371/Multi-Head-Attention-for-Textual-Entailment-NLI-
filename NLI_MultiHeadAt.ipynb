{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dac727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "Torch: 2.9.0+cu128 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — environment check and imports\n",
    "# Run this to ensure required libs are available and to import everything we'll use.\n",
    "import sys, math, os\n",
    "import torch\n",
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "print(\"Torch:\", getattr(torch, \"__version__\", \"n/a\"), \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a309874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: dict_keys(['test', 'validation', 'train'])\n",
      "Sizes: train 550152 validation 10000 test 10000\n",
      "\n",
      "Example 0:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is training his horse for a competition.\n",
      " Label: 1  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n",
      "\n",
      "Example 1:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is at a diner, ordering an omelette.\n",
      " Label: 2  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n",
      "\n",
      "Example 2:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is outdoors, on a horse.\n",
      " Label: 0  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — load SNLI and inspect raw examples\n",
    "snli = load_dataset(\"snli\")\n",
    "print(\"Splits:\", snli.keys())\n",
    "print(\"Sizes: train\", len(snli[\"train\"]), \"validation\", len(snli[\"validation\"]), \"test\", len(snli[\"test\"]))\n",
    "\n",
    "# show first 3 raw examples (these are plain python dicts)\n",
    "for i in range(3):\n",
    "    ex = snli[\"train\"][i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(\" Premise:\", ex[\"premise\"])\n",
    "    print(\" Hypothesis:\", ex[\"hypothesis\"])\n",
    "    print(\" Label:\", ex[\"label\"], \" (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bc0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts (train): Counter({0: 183416, 2: 183187, 1: 182764})\n",
      "Label counts (validation): Counter({0: 3329, 2: 3278, 1: 3235})\n",
      "Label counts (test): Counter({0: 3368, 2: 3237, 1: 3219})\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — filter invalid labels and look at label distribution\n",
    "snli = snli.filter(lambda ex: ex[\"label\"] is not None and ex[\"label\"] >= 0)\n",
    "from collections import Counter\n",
    "def label_counts(split):\n",
    "    return Counter([ex[\"label\"] for ex in snli[split]])\n",
    "print(\"Label counts (train):\", label_counts(\"train\"))\n",
    "print(\"Label counts (validation):\", label_counts(\"validation\"))\n",
    "print(\"Label counts (test):\", label_counts(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668d3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BertTokenizerFast\n",
      "Vocab size: 30522\n",
      "Pad token id: 0 Pad token: [PAD]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88415f59fba40bdb264c8933edf68c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'],\n",
      "        num_rows: 9842\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'],\n",
      "        num_rows: 549367\n",
      "    })\n",
      "})\n",
      "Columns: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — tokenizer: what it *does* and when it runs\n",
    "# We'll use a transformers tokenizer (subword/BERT-style). It converts text -> token ids and creates attention mask.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "print(\"Tokenizer:\", tokenizer.__class__.__name__)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token id:\", tokenizer.pad_token_id, \"Pad token:\", tokenizer.pad_token)\n",
    "\n",
    "# 2️⃣  Decide maximum length\n",
    "#    SNLI sentences are short, 64 tokens is plenty\n",
    "max_len = 64\n",
    "\n",
    "# 3️⃣  Define a function that tokenizes both premise & hypothesis separately\n",
    "def tokenize_pair(batch):\n",
    "    premise = tokenizer(batch[\"premise\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    hypo    = tokenizer(batch[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    return {\n",
    "        \"premise_input_ids\": premise[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise[\"attention_mask\"],\n",
    "        \"hypo_input_ids\": hypo[\"input_ids\"],\n",
    "        \"hypo_attention_mask\": hypo[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 4. apply map BUT keep the label column by removing everything EXCEPT 'label'\n",
    "orig_cols = snli[\"train\"].column_names\n",
    "cols_to_remove = [c for c in orig_cols if c != \"label\"]  # remove all except label\n",
    "snli_tok = snli.map(tokenize_pair, batched=True, remove_columns=cols_to_remove)\n",
    "\n",
    "print(snli_tok)\n",
    "print(\"Columns:\", snli_tok[\"train\"].column_names)\n",
    "\n",
    "# NOTE: tokenizers map text -> ids (and produce masks). This step is CPU work (fast with 'use_fast').\n",
    "# You can run this once for the whole dataset (pre-tokenize) or run it each batch (on-the-fly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c850af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'])\n",
      "premise_input_ids shape: torch.Size([32, 64])\n",
      "premise_attention_mask shape: torch.Size([32, 64])\n",
      "label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — set dataset to return PyTorch tensors and create a DataLoader (fixed-length prepadding)\n",
    "snli_tok.set_format(type=\"torch\", columns=[\n",
    "    \"premise_input_ids\", \"premise_attention_mask\",\n",
    "    \"hypo_input_ids\", \"hypo_attention_mask\", \"label\"\n",
    "])\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(snli_tok[\"train\"], batch_size=32, shuffle=True)\n",
    "\n",
    "# inspect one batch\n",
    "batch = next(iter(train_dl))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"premise_input_ids shape:\", batch[\"premise_input_ids\"].shape)  # (B, L)\n",
    "print(\"premise_attention_mask shape:\", batch[\"premise_attention_mask\"].shape)\n",
    "print(\"label shape:\", batch[\"label\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9da5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise tokens: ['[CLS]', 'a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', '[SEP]']\n",
      "Hypothesis tokens: ['[CLS]', 'a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.', '[SEP]']\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "sample = snli_tok[\"train\"][0]\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "# convert IDs back to tokens (stop at first PAD)\n",
    "prem_ids = sample[\"premise_input_ids\"].tolist()\n",
    "hypo_ids = sample[\"hypo_input_ids\"].tolist()\n",
    "\n",
    "prem_tokens = tokenizer.convert_ids_to_tokens(\n",
    "    prem_ids[:prem_ids.index(pad_id)] if pad_id in prem_ids else prem_ids\n",
    ")\n",
    "hypo_tokens = tokenizer.convert_ids_to_tokens(\n",
    "    hypo_ids[:hypo_ids.index(pad_id)] if pad_id in hypo_ids else hypo_ids\n",
    ")\n",
    "\n",
    "print(\"Premise tokens:\", prem_tokens)\n",
    "print(\"Hypothesis tokens:\", hypo_tokens)\n",
    "print(\"Label:\", sample[\"label\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
