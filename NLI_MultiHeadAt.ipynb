{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dac727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "Torch: 2.9.0+cu128 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — environment check and imports\n",
    "# Run this to ensure required libs are available and to import everything we'll use.\n",
    "import sys, math, os\n",
    "import torch\n",
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "print(\"Torch:\", getattr(torch, \"__version__\", \"n/a\"), \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a309874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: dict_keys(['test', 'validation', 'train'])\n",
      "Sizes: train 550152 validation 10000 test 10000\n",
      "\n",
      "Example 0:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is training his horse for a competition.\n",
      " Label: 1  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n",
      "\n",
      "Example 1:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is at a diner, ordering an omelette.\n",
      " Label: 2  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n",
      "\n",
      "Example 2:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is outdoors, on a horse.\n",
      " Label: 0  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — load SNLI and inspect raw examples\n",
    "snli = load_dataset(\"snli\")\n",
    "print(\"Splits:\", snli.keys())\n",
    "print(\"Sizes: train\", len(snli[\"train\"]), \"validation\", len(snli[\"validation\"]), \"test\", len(snli[\"test\"]))\n",
    "\n",
    "# show first 3 raw examples (these are plain python dicts)\n",
    "for i in range(3):\n",
    "    ex = snli[\"train\"][i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(\" Premise:\", ex[\"premise\"])\n",
    "    print(\" Hypothesis:\", ex[\"hypothesis\"])\n",
    "    print(\" Label:\", ex[\"label\"], \" (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bc0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts (train): Counter({0: 183416, 2: 183187, 1: 182764})\n",
      "Label counts (validation): Counter({0: 3329, 2: 3278, 1: 3235})\n",
      "Label counts (test): Counter({0: 3368, 2: 3237, 1: 3219})\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — filter invalid labels and look at label distribution\n",
    "snli = snli.filter(lambda ex: ex[\"label\"] is not None and ex[\"label\"] >= 0)\n",
    "from collections import Counter\n",
    "def label_counts(split):\n",
    "    return Counter([ex[\"label\"] for ex in snli[split]])\n",
    "print(\"Label counts (train):\", label_counts(\"train\"))\n",
    "print(\"Label counts (validation):\", label_counts(\"validation\"))\n",
    "print(\"Label counts (test):\", label_counts(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668d3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BertTokenizerFast\n",
      "Vocab size: 30522\n",
      "Pad token id: 0 Pad token: [PAD]\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'],\n",
      "        num_rows: 9842\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'],\n",
      "        num_rows: 549367\n",
      "    })\n",
      "})\n",
      "Columns: ['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — tokenizer: what it *does* and when it runs\n",
    "# We'll use a transformers tokenizer (subword/BERT-style). It converts text -> token ids and creates attention mask.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "print(\"Tokenizer:\", tokenizer.__class__.__name__)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token id:\", tokenizer.pad_token_id, \"Pad token:\", tokenizer.pad_token)\n",
    "\n",
    "# 2️⃣  Decide maximum length\n",
    "#    SNLI sentences are short, 64 tokens is plenty\n",
    "max_len = 64\n",
    "\n",
    "# 3️⃣  Define a function that tokenizes both premise & hypothesis separately\n",
    "def tokenize_pair(batch):\n",
    "    premise = tokenizer(batch[\"premise\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    hypo    = tokenizer(batch[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    return {\n",
    "        \"premise_input_ids\": premise[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise[\"attention_mask\"],\n",
    "        \"hypo_input_ids\": hypo[\"input_ids\"],\n",
    "        \"hypo_attention_mask\": hypo[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 4. apply map BUT keep the label column by removing everything EXCEPT 'label'\n",
    "orig_cols = snli[\"train\"].column_names\n",
    "cols_to_remove = [c for c in orig_cols if c != \"label\"]  # remove all except label\n",
    "snli_tok = snli.map(tokenize_pair, batched=True, remove_columns=cols_to_remove)\n",
    "\n",
    "print(snli_tok)\n",
    "print(\"Columns:\", snli_tok[\"train\"].column_names)\n",
    "\n",
    "# NOTE: tokenizers map text -> ids (and produce masks). This step is CPU work (fast with 'use_fast').\n",
    "# You can run this once for the whole dataset (pre-tokenize) or run it each batch (on-the-fly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c850af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['label', 'premise_input_ids', 'premise_attention_mask', 'hypo_input_ids', 'hypo_attention_mask'])\n",
      "premise_input_ids shape: torch.Size([32, 64])\n",
      "premise_attention_mask shape: torch.Size([32, 64])\n",
      "label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — set dataset to return PyTorch tensors and create a DataLoader (fixed-length prepadding)\n",
    "snli_tok.set_format(type=\"torch\", columns=[\n",
    "    \"premise_input_ids\", \"premise_attention_mask\",\n",
    "    \"hypo_input_ids\", \"hypo_attention_mask\", \"label\"\n",
    "])\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(snli_tok[\"train\"], batch_size=32, shuffle=True)\n",
    "\n",
    "# inspect one batch\n",
    "batch = next(iter(train_dl))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"premise_input_ids shape:\", batch[\"premise_input_ids\"].shape)  # (B, L)\n",
    "print(\"premise_attention_mask shape:\", batch[\"premise_attention_mask\"].shape)\n",
    "print(\"label shape:\", batch[\"label\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9da5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise tokens: ['[CLS]', 'a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', '[SEP]']\n",
      "Hypothesis tokens: ['[CLS]', 'a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.', '[SEP]']\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "sample = snli_tok[\"train\"][0]\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "# convert IDs back to tokens (stop at first PAD)\n",
    "prem_ids = sample[\"premise_input_ids\"].tolist()\n",
    "hypo_ids = sample[\"hypo_input_ids\"].tolist()\n",
    "\n",
    "prem_tokens = tokenizer.convert_ids_to_tokens(\n",
    "    prem_ids[:prem_ids.index(pad_id)] if pad_id in prem_ids else prem_ids\n",
    ")\n",
    "hypo_tokens = tokenizer.convert_ids_to_tokens(\n",
    "    hypo_ids[:hypo_ids.index(pad_id)] if pad_id in hypo_ids else hypo_ids\n",
    ")\n",
    "\n",
    "print(\"Premise tokens:\", prem_tokens)\n",
    "print(\"Hypothesis tokens:\", hypo_tokens)\n",
    "print(\"Label:\", sample[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb61fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# --- Positional Encoding (sinusoidal) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# --- MultiHead Attention implemented manually ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.qkv_layer = nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        B, L, D = X.shape\n",
    "        qkv = self.qkv_layer(X)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        self.attention_weights = attn.detach()\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        return self.output_layer(out)\n",
    "\n",
    "# --- NLI Model ---\n",
    "class NLIModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, pad_idx):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.pos_enc = PositionalEncoding(embedding_dim)\n",
    "        self.encoder = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "    def encode(self, input_ids, mask):\n",
    "        emb = self.embedding(input_ids)\n",
    "        emb = self.pos_enc(emb)\n",
    "        out = self.encoder(emb, mask.unsqueeze(1).unsqueeze(1))\n",
    "        pooled = (out * mask.unsqueeze(-1)).sum(dim=1) / mask.sum(dim=1, keepdim=True)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, premise_ids, premise_mask, hypo_ids, hypo_mask):\n",
    "        prem_vec = self.encode(premise_ids, premise_mask)\n",
    "        hypo_vec = self.encode(hypo_ids, hypo_mask)\n",
    "        combined = torch.cat([prem_vec, hypo_vec], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
