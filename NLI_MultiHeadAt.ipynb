{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dac727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "Torch: 2.9.0+cu128 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — environment check and imports\n",
    "# Run this to ensure required libs are available and to import everything we'll use.\n",
    "import sys, math, os\n",
    "import torch\n",
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "print(\"Torch:\", getattr(torch, \"__version__\", \"n/a\"), \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a309874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: dict_keys(['test', 'validation', 'train'])\n",
      "Sizes: train 550152 validation 10000 test 10000\n",
      "\n",
      "Example 0:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is training his horse for a competition.\n",
      " Label: 1  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n",
      "\n",
      "Example 1:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is at a diner, ordering an omelette.\n",
      " Label: 2  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n",
      "\n",
      "Example 2:\n",
      " Premise: A person on a horse jumps over a broken down airplane.\n",
      " Hypothesis: A person is outdoors, on a horse.\n",
      " Label: 0  (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — load SNLI and inspect raw examples\n",
    "snli = load_dataset(\"snli\")\n",
    "print(\"Splits:\", snli.keys())\n",
    "print(\"Sizes: train\", len(snli[\"train\"]), \"validation\", len(snli[\"validation\"]), \"test\", len(snli[\"test\"]))\n",
    "\n",
    "# show first 3 raw examples (these are plain python dicts)\n",
    "for i in range(3):\n",
    "    ex = snli[\"train\"][i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(\" Premise:\", ex[\"premise\"])\n",
    "    print(\" Hypothesis:\", ex[\"hypothesis\"])\n",
    "    print(\" Label:\", ex[\"label\"], \" (0=entailment, 1=neutral, 2=contradiction; -1 may mean missing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bc0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts (train): Counter({0: 183416, 2: 183187, 1: 182764})\n",
      "Label counts (validation): Counter({0: 3329, 2: 3278, 1: 3235})\n",
      "Label counts (test): Counter({0: 3368, 2: 3237, 1: 3219})\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — filter invalid labels and look at label distribution\n",
    "snli = snli.filter(lambda ex: ex[\"label\"] is not None and ex[\"label\"] >= 0)\n",
    "from collections import Counter\n",
    "def label_counts(split):\n",
    "    return Counter([ex[\"label\"] for ex in snli[split]])\n",
    "print(\"Label counts (train):\", label_counts(\"train\"))\n",
    "print(\"Label counts (validation):\", label_counts(\"validation\"))\n",
    "print(\"Label counts (test):\", label_counts(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668d3dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff961ce9ca3474e822080c426e052a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8f18b5625249e088c544770eaeb432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d977e02e0c41aca82acda2b49f5511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62534060cd694899bd181c6b32a3b543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BertTokenizerFast\n",
      "Vocab size: 30522\n",
      "Pad token id: 0 Pad token: [PAD]\n",
      "\n",
      "Encoded keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "input_ids (length): 32\n",
      "input_ids (first 20): [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2731, 2010, 3586]\n",
      "attention_mask (first 20): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "token strings (first 20): ['[CLS]', 'a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', '[SEP]', 'a', 'person', 'is', 'training', 'his', 'horse']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — tokenizer: what it *does* and when it runs\n",
    "# We'll use a transformers tokenizer (subword/BERT-style). It converts text -> token ids and creates attention mask.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "print(\"Tokenizer:\", tokenizer.__class__.__name__)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token id:\", tokenizer.pad_token_id, \"Pad token:\", tokenizer.pad_token)\n",
    "\n",
    "# Example: tokenization pipeline on ONE pair (instant demonstration)\n",
    "sample = snli[\"train\"][0]\n",
    "enc = tokenizer(sample[\"premise\"], sample[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "print(\"\\nEncoded keys:\", list(enc.keys()))\n",
    "print(\"input_ids (length):\", len(enc[\"input_ids\"]))\n",
    "print(\"input_ids (first 20):\", enc[\"input_ids\"][:20])\n",
    "print(\"attention_mask (first 20):\", enc[\"attention_mask\"][:20])\n",
    "print(\"token strings (first 20):\", tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])[:20])\n",
    "\n",
    "# NOTE: tokenizers map text -> ids (and produce masks). This step is CPU work (fast with 'use_fast').\n",
    "# You can run this once for the whole dataset (pre-tokenize) or run it each batch (on-the-fly).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
